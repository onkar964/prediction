Overview
This project is to select a prediction model for predicting 20 test cases from a test dataset.
Dataset Preparation
# Load the libraries and datasets
library(caret)
## Warning: package 'caret' was built under R version 3.4.2
## Loading required package: lattice
## Loading required package: ggplot2
library(rpart)
library(randomForest)
## Warning: package 'randomForest' was built under R version 3.4.3
## randomForest 4.6-12
## Type rfNews() to see new features/changes/bug fixes.
## 
## Attaching package: 'randomForest'
## The following object is masked from 'package:ggplot2':
## 
##     margin
library(e1071)
## Warning: package 'e1071' was built under R version 3.4.3
library(gbm)
## Warning: package 'gbm' was built under R version 3.4.3
## Loading required package: survival
## 
## Attaching package: 'survival'
## The following object is masked from 'package:caret':
## 
##     cluster
## Loading required package: splines
## Loading required package: parallel
## Loaded gbm 2.1.3
set.seed(1234)

# Set the URLs
TrainDataURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
TestDataURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

# Download and clean the datasets
TrainingData <- read.csv(url(TrainDataURL), na.strings=c("NA","#DIV/0!",""))
TestingData <- read.csv(url(TestDataURL), na.strings=c("NA","#DIV/0!",""))

# Check for the datasets dimemsions
dim(TrainingData)
## [1] 19622   160
dim(TestingData)
## [1]  20 160
# Delete columns with missing values
TrainingData <-TrainingData[,colSums(is.na(TrainingData)) == 0]
TestingData <-TestingData[,colSums(is.na(TestingData)) == 0]

# Delete unused columns
TrainingData <-TrainingData[,-c(1:7)]
TestingData <-TestingData[,-c(1:7)]

# Check for the datasets dimemsions
dim(TrainingData)
## [1] 19622    53
dim(TestingData)
## [1] 20 53
Dataset Partitioning
# Partitions training dataset
PartData <- createDataPartition(TrainingData$classe, p=0.7, list=FALSE)
TrainingSet <- TrainingData[PartData, ]
TestingSet <- TrainingData[-PartData, ]
dim(TrainingSet)
## [1] 13737    53
dim(TestingSet)
## [1] 5885   53
Examine the Training Dataset by Showing its Frequency
plot(TrainingSet$classe, col="black", main="Frequency of different levels", xlab="classe", ylab="Frequency")

Based on the frequency distribution, it shows that each level frequency is within the same order of magnitude of each other. Level A is the most frequent, and level D is the least frequent.
Prediction using Random Forest Model and Generalized Boosted Model
1. Random Forest
# Fitting model
model1 <- randomForest(classe ~., data=TrainingSet, method="class")

# Predicting
prediction1 <- predict(model1, TestingSet, Type="class")

# Testing
confusionMatrix(prediction1, TestingSet$classe)
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1674    7    0    0    0
##          B    0 1131    6    0    0
##          C    0    1 1020    5    0
##          D    0    0    0  958    1
##          E    0    0    0    1 1081
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9964          
##                  95% CI : (0.9946, 0.9978)
##     No Information Rate : 0.2845          
##     P-Value [Acc > NIR] : < 2.2e-16       
##                                           
##                   Kappa : 0.9955          
##  Mcnemar's Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            1.0000   0.9930   0.9942   0.9938   0.9991
## Specificity            0.9983   0.9987   0.9988   0.9998   0.9998
## Pos Pred Value         0.9958   0.9947   0.9942   0.9990   0.9991
## Neg Pred Value         1.0000   0.9983   0.9988   0.9988   0.9998
## Prevalence             0.2845   0.1935   0.1743   0.1638   0.1839
## Detection Rate         0.2845   0.1922   0.1733   0.1628   0.1837
## Detection Prevalence   0.2856   0.1932   0.1743   0.1630   0.1839
## Balanced Accuracy      0.9992   0.9959   0.9965   0.9968   0.9994
2. Generalized Boosted Model
# Fitting model
fitControl <- trainControl(method="repeatedcv", number=5, repeats=1)
model2 <- train(classe ~., data=TrainingSet, method="gbm", trControl=fitControl, verbose=FALSE)

# Predicting
prediction2 <- predict(model2, TestingSet)

# Testing
confusionMatrix(prediction2, TestingSet$classe)
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1648   44    0    0    1
##          B   17 1065   32    8   14
##          C    3   29  983   20    9
##          D    5    0   10  927   13
##          E    1    1    1    9 1045
## 
## Overall Statistics
##                                          
##                Accuracy : 0.9631         
##                  95% CI : (0.958, 0.9678)
##     No Information Rate : 0.2845         
##     P-Value [Acc > NIR] : < 2.2e-16      
##                                          
##                   Kappa : 0.9533         
##  Mcnemar's Test P-Value : 2.874e-07      
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9845   0.9350   0.9581   0.9616   0.9658
## Specificity            0.9893   0.9850   0.9874   0.9943   0.9975
## Pos Pred Value         0.9734   0.9375   0.9416   0.9707   0.9886
## Neg Pred Value         0.9938   0.9844   0.9911   0.9925   0.9923
## Prevalence             0.2845   0.1935   0.1743   0.1638   0.1839
## Detection Rate         0.2800   0.1810   0.1670   0.1575   0.1776
## Detection Prevalence   0.2877   0.1930   0.1774   0.1623   0.1796
## Balanced Accuracy      0.9869   0.9600   0.9728   0.9780   0.9817
The accuracy of Random Forest is 99.64, which is higher than that by Generalized Boosted. The Random Forest
model will be applied to predict the 20 quiz results.
predictTest <- predict(model1, TestingData)
predictTest
##  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 
##  B  A  B  A  A  E  D  B  A  A  B  C  B  A  E  E  A  B  B  B 
## Levels: A B C D E
